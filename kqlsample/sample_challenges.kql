//Data lands in the StageIoTRaw table so lets look at that first
.show table StageIoTRawData

.show table StageIoTRawData extents 
| summarize ExtentSize=format_bytes(sum(ExtentSize)), IngestionSize=format_bytes(sum(OriginalSize))

StageIoTRawData
| limit 10 

//We use an update policy to flatten this data and append it to the Thermostat table
.show table Thermostats

//Here is the update policy. Expand the Policy column to see the details
.show table Thermostats policy update 

//Here is the function that is used for the update policy
.show function ExtractThermostatData

Thermostats
| limit 10 

    // CHALLENGE 1: What's the min & max?

Thermostats
| summarize MinDate=min(FILLIN), MaxDate=max(FILLIN), MinIngest=min(ingestion_time())

    // CHALLENGE 2: What's the min difference? 

// There should be a little over 30K rows in this table
Thermostats
| count

    // CHALLENGE 3: How do I see my DeviceIds? 

//What is the average temp every 1 min for the month of January?
Thermostats
| where EnqueuedTimeUTC between (datetime(2022-01-01) .. datetime(2022-01-31))
| where DeviceId == '637086755190714287'
| summarize avg(Temp) by bin(EnqueuedTimeUTC,1m)
| render timechart 

//Same query but lets look at all thermostats in 1 hour aggregates for January
Thermostats
| where EnqueuedTimeUTC between (datetime(2022-2-22) .. datetime(2022-02-27))
| summarize avg(Temp) by bin(EnqueuedTimeUTC,1h), DeviceId
| render timechart 

    // CHALLENGE 4: How do I set a dynamic date? 

//Is there any missing data? 
//make-series
//Create series of specified aggregated values along specified axis.
let start = FILLIN ;
let end = FILLIN ;
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| where DeviceId == '637086755190714287'
| make-series AvgTemp=avg(Temp) on EnqueuedTimeUTC from start to end step 1m   
| render timechart 

//How can I fill the missing values?
//series_fill_linear()
//Performs linear interpolation of missing values in a series.
let start = datetime(2022-01-01);
let end = datetime(2022-01-04);
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| where DeviceId == '637086755190714287'
| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from start to end step 1m
| extend NoGapsTemp=series_fill_linear(AvgTemp)
| project EnqueuedTimeUTC, NoGapsTemp
| render timechart 

//What will be the temprature for next one hour? Note that we are using historical data so we will use 1-31 as present 
let start = datetime(2022-01-29);
let end = datetime(2022-01-31);
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| where DeviceId == '637086755190714287'
| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from start to end+1h step 1m   
| extend NoGapsTemp=series_fill_linear(AvgTemp)
| project EnqueuedTimeUTC, NoGapsTemp
| extend forecast = series_decompose_forecast(NoGapsTemp, 60)
| render timechart with(title='Forecasting the next 1hr by Time Series Decmposition')


    // CHALLENGE 4: What's the temperature going to be in 12 hours? 

//Lets forcast out 12 hours
let start = datetime(2022-01-29);
let end = datetime(2022-01-31);
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| where DeviceId == '637086755190714287'
| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from start to end+FILLIN step 1m 
| extend NoGapsTemp=series_fill_linear(AvgTemp)
| project EnqueuedTimeUTC, NoGapsTemp
| extend forecast = series_decompose_forecast(NoGapsTemp, FILLIN) // n hours * 60
| render timechart with(title='Forecasting by Time Series Decmposition')

//Are there any anomalies for this device?
let start = datetime(2022-01-29);
let end = datetime(2022-01-31);
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| where DeviceId == '637086755190714287'
| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from start to end step 1m   
| extend NoGapsTemp=series_fill_linear(AvgTemp)
| project EnqueuedTimeUTC, NoGapsTemp
| extend anomalies = series_decompose_anomalies(NoGapsTemp,1) 
| render anomalychart with(anomalycolumns=anomalies)

//Lets make it less sensative
let start = datetime(2022-01-29);
let end = datetime(2022-01-31);
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| where DeviceId == '637086755190714287'
| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from start to end step 1m   
| extend NoGapsTemp=series_fill_linear(AvgTemp)
| project EnqueuedTimeUTC, NoGapsTemp
| extend anomalies = series_decompose_anomalies(NoGapsTemp,1.5) 
| render anomalychart with(anomalycolumns=anomalies)

//What anomalies should I focus on across all devices?
let start = datetime(2022-01-29);
let end = datetime(2022-01-31);
Thermostats
| where EnqueuedTimeUTC between (start .. end)
| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from start to end step 1m by DeviceId
| extend NoGapsTemp=series_fill_linear(AvgTemp)
| project EnqueuedTimeUTC, DeviceId, NoGapsTemp
| extend anomalies = series_decompose_anomalies(NoGapsTemp, 1.5)
| mv-expand EnqueuedTimeUTC, anomalies, NoGapsTemp
| where anomalies == 1

//Lets shift focus to the current data being ingested into ADX

//We have the ability to query out to blob, sql, cosmos, adt (azure digital twins), etc..
//The provisioning process created a function that will query out to ADT so lets look at that

//This function allows us to get the thermostats at certain offices (Atlanta, Dallas, or Seattle). 
//The metadata includes the floor the thermostat is on
.show function GetDevicesbyStore

//Here is an example of the query. Note that the below ADT endpoint won't match yours
//.create-or-alter function with (folder = "Analytics/IoT", skipvalidation = "true") GetDevicesbyStore(Office:string) 
//{
//	let ADTendpoint = "https://digitaltwinpm24.api.eus.digitaltwins.azure.net";
//	let ADTquery = strcat("SELECT T.$dtId as Office, F.$dtId as Floor, D.$dtId as DeviceId FROM DIGITALTWINS T JOIN F RELATED T.officecontainsfloors JOIN D RELATED F.floorcontainsdevices where T.$dtId='", Office, "'");
//	evaluate azure_digital_twins_query_request(ADTendpoint, ADTquery)
//    | project Office=tostring(Office), Floor=tostring(Floor), DeviceId=tostring(DeviceId)
//}

//Lets get the devices in the Dallas office
GetDevicesbyStore('Dallas')

//Now lets join that with ADX data and chart the results. We combine the floor from ADT with the DeviceId to ad context
GetDevicesbyStore('Dallas')
| join kind=inner (Thermostats
    | where EnqueuedTimeUTC >= ago(1h)
    | summarize Temp=avg(Temp) by DeviceId, AggTime=bin(EnqueuedTimeUTC, 1m)
) on DeviceId
    // CHALLENGE 5: Can we render the 1st Floor only?
| extend DeviceId=strcat(Floor, '-', DeviceId)
| project todouble(Temp), AggTime, DeviceId
| render timechart  with  (ycolumns = Temp, series=DeviceId)

//Materialized views are commonly used to
// a. Store aggregates of the raw data
// b. Store last know values
// c. Remove duplicate records

// Lets create two materialized views. One for hourly aggregates and one for last known values
// We'll also backfill the records. You will typically use async but for this sample we'll wait for it to complete
.create materialized-view with (backfill=true) Hourly_Average_Mview on table Thermostats
{
    Thermostats
    | summarize Temp=avg(Temp), BatteryLevel=avg(BatteryLevel), Humidity=avg(Humidity) by DeviceId, TimeStamp=bin(EnqueuedTimeUTC, 1h) 
} 

Hourly_Average_Mview
| where TimeStamp > ago(1h)
| take 1000 

// Now lets create a current view
.create materialized-view with (backfill=true) Current_Mview on table Thermostats
{
    Thermostats
    | summarize arg_max(EnqueuedTimeUTC, *) by DeviceId 
} 

    // CHALLENGE 6: How many devices?

Current_Mview


// ML : Lets shift focus to get additional insights of data being ingested into ADX.

// Here's a pretrained model.

datatable (name: string, timestamp: datetime, model: string) [
    "Occupancy",datetime(2019-11-05T15:28:53.010134Z),"800363736b6c6561726e2e6c696e6561725f6d6f64656c2e6c6f6769737469630a4c6f67697374696352656772657373696f6e0a7100298171017d710228580700000070656e616c7479710358020000006c32710458040000006475616c7105895803000000746f6c7106473f1a36e2eb1c432d5801000000437107473ff0000000000000580d0000006669745f696e746572636570747108885811000000696e746572636570745f7363616c696e6771094b01580c000000636c6173735f776569676874710a4e580c00000072616e646f6d5f7374617465710b4e5806000000736f6c766572710c58090000006c69626c696e656172710d58080000006d61785f69746572710e4b64580b0000006d756c74695f636c617373710f58040000007761726e71105807000000766572626f736571114b00580a0000007761726d5f737461727471128958060000006e5f6a6f627371134e5808000000636c61737365735f7114636e756d70792e636f72652e6d756c746961727261790a5f7265636f6e7374727563740a7115636e756d70790a6e6461727261790a71164b00857117430162711887711952711a284b014b0285711b636e756d70790a64747970650a711c58020000006231711d4b004b0187711e52711f284b0358010000007c71204e4e4e4affffffff4affffffff4b007471216289430200017122747123625805000000636f65665f7124681568164b008571256818877126527127284b014b014b05867128681c5802000000663871294b004b0187712a52712b284b0358010000003c712c4e4e4e4affffffff4affffffff4b0074712d6289432883ebdfd50687e0bf2cdaca74fa93a63fd3abc0080e6e943f650656defdad713f18f6a86bd73202bf712e74712f62580a000000696e746572636570745f7130681568164b008571316818877132527133284b014b01857134682b89430808459f57711290bf71357471366258070000006e5f697465725f7137681568164b00857138681887713952713a284b014b0185713b681c58020000006934713c4b004b0187713d52713e284b03682c4e4e4e4affffffff4affffffff4b0074713f628943040c00000071407471416258100000005f736b6c6561726e5f76657273696f6e71425806000000302e32302e33714375622e"
]

    // CHALLENGE 7: How to load it...select into table?

.FILLIN ML_Models FILLIN

ML_Models


// Python UnSupervised Learning 
// Here we do not label data to train the model. Labelling means to classify into different categories. Labelling mainly takes place in supervised learning.
// Create a custom UDF to run K-Means clustering using Python plugin
.create-or-alter function with (folder = "Python") kmeans_sf_OccupDetc(tbl:(*),k:int,features:dynamic,cluster_col:string) {
    let kwargs = pack('k', k, 'features', features, 'cluster_col', cluster_col);
    let code =
        '\n'
        'from sklearn.cluster import KMeans\n'
        '\n'
        'k = kargs["k"]\n'
        'features = kargs["features"]\n'
        'cluster_col = kargs["cluster_col"]\n'
        '\n'
        'km = KMeans(n_clusters=k)\n'
        'df1 = df[features]\n'
        'km.fit(df1)\n'
        'result = df\n'
        'result[cluster_col] = km.labels_\n';
    tbl
    | evaluate python(typeof(*), code, kwargs)
}


// Invoke the custom UDF for KMeans clusters
Thermostats
| where EnqueuedTimeUTC > ago(7d)
| extend cluster_id=double(null)
| project EnqueuedTimeUTC, DeviceId, Temp, Humidity, cluster_id
| invoke kmeans_sf_OccupDetc(3, pack_array("Temp", "Humidity"), "cluster_id")
| sample 10


// Python Supervised Model
// Custom UDF to score based on pre-trained model
.create-or-alter function with (folder = "Python", skipvalidation = "true") classify_sf_OccupDetc(samples:(*), models_tbl:(name:string,timestamp:datetime,model:string), model_name:string, features_cols:dynamic, pred_col:string) {
    let model_str = toscalar(ML_Models | where name == model_name | top 1 by timestamp desc | project model);
    let kwargs = pack('smodel', model_str, 'features_cols', features_cols, 'pred_col', pred_col);
    let code =
    '\n'
    'import pickle\n'
    'import binascii\n'
    '\n'
    'smodel = kargs["smodel"]\n'
    'features_cols = kargs["features_cols"]\n'
    'pred_col = kargs["pred_col"]\n'
    'bmodel = binascii.unhexlify(smodel)\n'
    'clf1 = pickle.loads(bmodel)\n'
    'df1 = df[features_cols]\n'
    'predictions = clf1.predict(df1)\n'
    '\n'
    'result = df\n'
    'result[pred_col] = pd.DataFrame(predictions, columns=[pred_col])'
    '\n'
    ;
    samples | evaluate python(typeof(*), code, kwargs)
} 


//Based on the Temp and Humidity - Is the room occupied?
Thermostats
| where EnqueuedTimeUTC > ago(15m)
| extend pred_Occupancy=bool(0)
| extend CO2=0, HumidityRatio=0
| invoke classify_sf_OccupDetc(ML_Models, 'Occupancy', pack_array('Temp', 'Humidity', 'BatteryLevel', 'CO2', 'HumidityRatio'), 'pred_Occupancy')
    

    // CHALLENGE 8: What is the count of occupied rooms for last 8 hours?

Thermostats
| where EnqueuedTimeUTC > ago(FILLIN)
| extend pred_Occupancy=bool(0)
| extend CO2=0, HumidityRatio=0
| invoke classify_sf_OccupDetc(ML_Models, 'Occupancy', pack_array('Temp', 'Humidity', 'BatteryLevel', 'CO2', 'HumidityRatio'), 'pred_Occupancy')
| where pred_Occupancy=FILLIN
| FILLIN
